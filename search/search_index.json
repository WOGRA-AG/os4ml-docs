{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Open Space for Machine Learning Open space for Machine Learning ( Os4ML ) is an open source platform for automated machine learning. The goal is to enable non-experts to solve every day problems with AI. It automates all steps on the way to the finished AI model with the help of an intuitive UI/UX. About the Project The project focuses on easy installation, intuitive UI/UX and comfortable machine learning. We're not reinventing the wheel. Whenever possible, we use third-party open source software. For example, we rely on Kubeflow , especially for machine learning tasks. But don't worry, you don't need a running Kubeflow. Kubeflow runs under the hood at Os4ML. The project is in an early phase, i.e. everything is work in progress and experimental. We plan to release a first demo version at the end of March. License Os4ML is primarily distributed under the terms of both the MIT license and the Apache License (Version 2.0). See LICENSE-APACHE , LICENSE-MIT , and COPYRIGHT for details.","title":"Home"},{"location":"#welcome-to-open-space-for-machine-learning","text":"Open space for Machine Learning ( Os4ML ) is an open source platform for automated machine learning. The goal is to enable non-experts to solve every day problems with AI. It automates all steps on the way to the finished AI model with the help of an intuitive UI/UX.","title":"Welcome to Open Space for Machine Learning"},{"location":"#about-the-project","text":"The project focuses on easy installation, intuitive UI/UX and comfortable machine learning. We're not reinventing the wheel. Whenever possible, we use third-party open source software. For example, we rely on Kubeflow , especially for machine learning tasks. But don't worry, you don't need a running Kubeflow. Kubeflow runs under the hood at Os4ML. The project is in an early phase, i.e. everything is work in progress and experimental. We plan to release a first demo version at the end of March.","title":"About the Project"},{"location":"#license","text":"Os4ML is primarily distributed under the terms of both the MIT license and the Apache License (Version 2.0). See LICENSE-APACHE , LICENSE-MIT , and COPYRIGHT for details.","title":"License"},{"location":"about_us/","text":"About Us Os4ML is a project of the WOGRA AG research group in cooperation with the German Aerospace Center and is funded by the Ministry of Economic Affairs, Regional Development and Energy as part of the High Tech Agenda of the Free State of Bavaria.","title":"About Us"},{"location":"about_us/#about-us","text":"Os4ML is a project of the WOGRA AG research group in cooperation with the German Aerospace Center and is funded by the Ministry of Economic Affairs, Regional Development and Energy as part of the High Tech Agenda of the Free State of Bavaria.","title":"About Us"},{"location":"design/","text":"Design Service Architecture Cluster Architecture","title":"Design"},{"location":"design/#design","text":"","title":"Design"},{"location":"design/#service-architecture","text":"","title":"Service Architecture"},{"location":"design/#cluster-architecture","text":"","title":"Cluster Architecture"},{"location":"getting_started/","text":"Getting Started We try to keep things as simple as possible. Prerequisites In fact, all it takes is a running Kubernetes cluster to get started. With k3d you can do it like this, for example k3d cluster create os4ml-cluster If you want to do machine learning, you want to use GPUs. Os4ML uses automatically uses GPUs if they are available in the Kubernetes cluster. Unfortunately, GPUs are known to be a topic of their own. Nonetheless, we provide some helpful information on how to set up a kubernetes cluster with gpu support. Please read the support for k3d or the support for MicroK8s for more details. Installation Nobody wants complicated installations. We neither. For this reason we provide Terraform scripts to install Os4ML on any Kubernetes cluster. And this is how it works: git clone https://github.com/WOGRA-AG/Os4ML.git cd os4ml/terraform terraform init terraform apply -auto-approve This takes a bit and offers the opportunity to get a coffee. Usage You won't believe it, but in fact your Kubernetes cluster now hosts Os4ML including a fully functional Kubeflow . And that's how you get it: kubectl port-forward -n istio-system svc/istio-ingressgateway 8000:80 Now, open localhost:8000 for Kubeflow or localhost:8000/os4ml/ for Os4ML (don't forget the slash). As described here , the connection is terminated when the command is aborted. Whenever you are asked for credentials, there exists a standard user with email user@example.com and password 12341234 .","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"We try to keep things as simple as possible.","title":"Getting Started"},{"location":"getting_started/#prerequisites","text":"In fact, all it takes is a running Kubernetes cluster to get started. With k3d you can do it like this, for example k3d cluster create os4ml-cluster If you want to do machine learning, you want to use GPUs. Os4ML uses automatically uses GPUs if they are available in the Kubernetes cluster. Unfortunately, GPUs are known to be a topic of their own. Nonetheless, we provide some helpful information on how to set up a kubernetes cluster with gpu support. Please read the support for k3d or the support for MicroK8s for more details.","title":"Prerequisites"},{"location":"getting_started/#installation","text":"Nobody wants complicated installations. We neither. For this reason we provide Terraform scripts to install Os4ML on any Kubernetes cluster. And this is how it works: git clone https://github.com/WOGRA-AG/Os4ML.git cd os4ml/terraform terraform init terraform apply -auto-approve This takes a bit and offers the opportunity to get a coffee.","title":"Installation"},{"location":"getting_started/#usage","text":"You won't believe it, but in fact your Kubernetes cluster now hosts Os4ML including a fully functional Kubeflow . And that's how you get it: kubectl port-forward -n istio-system svc/istio-ingressgateway 8000:80 Now, open localhost:8000 for Kubeflow or localhost:8000/os4ml/ for Os4ML (don't forget the slash). As described here , the connection is terminated when the command is aborted. Whenever you are asked for credentials, there exists a standard user with email user@example.com and password 12341234 .","title":"Usage"},{"location":"gpu_support/k3d/","text":"GPU support with k3d k3d says it's 'a lightweight wrapper to run k3s (Rancher Lab\u2019s minimal Kubernetes distribution) in docker'. Well yes, they are right, and they are great. This is the reason why we use k3d for our daily development work. For this reason it can be assumed that Os4ML runs stably on k3d . It's quite nice to equip k3d with gpu support as described here .If you have problems building the Docker image, remember to install the nvidia-container-toolkit in addition to the nvidia-container-runtime .","title":"k3d"},{"location":"gpu_support/k3d/#gpu-support-with-k3d","text":"k3d says it's 'a lightweight wrapper to run k3s (Rancher Lab\u2019s minimal Kubernetes distribution) in docker'. Well yes, they are right, and they are great. This is the reason why we use k3d for our daily development work. For this reason it can be assumed that Os4ML runs stably on k3d . It's quite nice to equip k3d with gpu support as described here .If you have problems building the Docker image, remember to install the nvidia-container-toolkit in addition to the nvidia-container-runtime .","title":"GPU support with k3d"},{"location":"gpu_support/microk8s/","text":"GPU support with MicroK8s Canonical promotes MicroK8s as 'Low-ops, minimal production Kubernetes, for devs, cloud, clusters, workstations, Edge and IoT'. GPU support is provided as add-on using th command microk8s enable gpu as described here . Unfortunately, this fails from time to time. In the following we describe an alternative procedure to enable gpu support for microk8s. Unfortunately, due to time constraints, the instructions cannot be continuously updated. Please let us know if something is no longer up to date so that this can be adjusted accordingly. Install MicroK8s Be sure no MicroK8s is installed, otherwise use sudo snap remove microk8s --purge . Install MicroK8s on Linux sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Enable DNS support microk8s enable dns Enable GPU support in microk8s Now we are adapting the following article . Add a demonset to the node which is responsible to enable gpu capacity (nvidia.com/gpu) for every node it is done once. microk8s kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.14/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml Add a label to the node for gpu support microk8s kubectl label nodes DEVICE_NAME cloud.google.com/gke-accelerator=true Bugfix containerd gpu support for microk8s To enable the nvidia-conatiner-runtime in containerd edit the two files sudo vim /var/snap/microk8s/current/args/containerd-template.toml sudo vim /var/snap/microk8s/current/args/containerd.toml And replace # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" with # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] BinaryName = \"nvidia-container-runtime\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" With this setting every container executed by containerd has gpu support, but that is for our use case fine. Ensure nvidia driver is loaded and device files are ready Use the script from here . touch enable-dev-nvida.sh vim enable-dev-nvida.sh Add this code #!/bin/bash /sbin/modprobe nvidia if [ \"$?\" -eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\" -eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi And execute the script chmod +x enable-dev-nvida.sh sudo ./enable-dev-nvida.sh Also check that Nvidia and Cuda is present nvidia-smi Sample output +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.63.01 Driver Version: 470.63.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:3B:00.0 Off | N/A | | N/A 54C P0 N/A / N/A | 249MiB / 2002MiB | 10% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Restart MicroK8s and get node information microk8s stop && microk8s start && microk8s status --wait-ready microk8s kubectl describe node You should now get nvidia.com/gpu capacity like this Capacity: cpu: 8 ephemeral-storage: 473841544Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16030004Ki nvidia.com/gpu: 1 pods: 110 Test pod with GPU capabilities To test the gpu support we create a simple deployment yaml file vim test-gpu.yaml with content apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 requests: nvidia.com/gpu: 1 Now start the deployment and look for the results. This can take some time to pull the image. sudo microk8s kubectl apply -f test-gpu.yaml microk8s kubectl describe pod cuda-vector-add microk8s kubectl logs cuda-vector-add The successful log output shows that the GPU is working and can now be used. [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done","title":"MicroK8s"},{"location":"gpu_support/microk8s/#gpu-support-with-microk8s","text":"Canonical promotes MicroK8s as 'Low-ops, minimal production Kubernetes, for devs, cloud, clusters, workstations, Edge and IoT'. GPU support is provided as add-on using th command microk8s enable gpu as described here . Unfortunately, this fails from time to time. In the following we describe an alternative procedure to enable gpu support for microk8s. Unfortunately, due to time constraints, the instructions cannot be continuously updated. Please let us know if something is no longer up to date so that this can be adjusted accordingly.","title":"GPU support with MicroK8s"},{"location":"gpu_support/microk8s/#install-microk8s","text":"Be sure no MicroK8s is installed, otherwise use sudo snap remove microk8s --purge . Install MicroK8s on Linux sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Enable DNS support microk8s enable dns","title":"Install MicroK8s"},{"location":"gpu_support/microk8s/#enable-gpu-support-in-microk8s","text":"Now we are adapting the following article . Add a demonset to the node which is responsible to enable gpu capacity (nvidia.com/gpu) for every node it is done once. microk8s kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.14/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml Add a label to the node for gpu support microk8s kubectl label nodes DEVICE_NAME cloud.google.com/gke-accelerator=true","title":"Enable GPU support in microk8s"},{"location":"gpu_support/microk8s/#bugfix-containerd-gpu-support-for-microk8s","text":"To enable the nvidia-conatiner-runtime in containerd edit the two files sudo vim /var/snap/microk8s/current/args/containerd-template.toml sudo vim /var/snap/microk8s/current/args/containerd.toml And replace # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" with # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] BinaryName = \"nvidia-container-runtime\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" With this setting every container executed by containerd has gpu support, but that is for our use case fine.","title":"Bugfix containerd gpu support for microk8s"},{"location":"gpu_support/microk8s/#ensure-nvidia-driver-is-loaded-and-device-files-are-ready","text":"Use the script from here . touch enable-dev-nvida.sh vim enable-dev-nvida.sh Add this code #!/bin/bash /sbin/modprobe nvidia if [ \"$?\" -eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\" -eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi And execute the script chmod +x enable-dev-nvida.sh sudo ./enable-dev-nvida.sh Also check that Nvidia and Cuda is present nvidia-smi Sample output +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.63.01 Driver Version: 470.63.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:3B:00.0 Off | N/A | | N/A 54C P0 N/A / N/A | 249MiB / 2002MiB | 10% Default | | | | N/A | +-------------------------------+----------------------+----------------------+","title":"Ensure nvidia driver is loaded and device files are ready"},{"location":"gpu_support/microk8s/#restart-microk8s-and-get-node-information","text":"microk8s stop && microk8s start && microk8s status --wait-ready microk8s kubectl describe node You should now get nvidia.com/gpu capacity like this Capacity: cpu: 8 ephemeral-storage: 473841544Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16030004Ki nvidia.com/gpu: 1 pods: 110","title":"Restart MicroK8s and get node information"},{"location":"gpu_support/microk8s/#test-pod-with-gpu-capabilities","text":"To test the gpu support we create a simple deployment yaml file vim test-gpu.yaml with content apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 requests: nvidia.com/gpu: 1 Now start the deployment and look for the results. This can take some time to pull the image. sudo microk8s kubectl apply -f test-gpu.yaml microk8s kubectl describe pod cuda-vector-add microk8s kubectl logs cuda-vector-add The successful log output shows that the GPU is working and can now be used. [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done","title":"Test pod with GPU capabilities"}]}