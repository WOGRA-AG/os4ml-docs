{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs Please, for full documentation visit mkdocs.org . Command mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"Please, for full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#command","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Command"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"design/","text":"Design Service Architecture Cluster Architecture","title":"Design"},{"location":"design/#design","text":"","title":"Design"},{"location":"design/#service-architecture","text":"","title":"Service Architecture"},{"location":"design/#cluster-architecture","text":"","title":"Cluster Architecture"},{"location":"started/","text":"Getting Started Install MicroK8s with GPU support Install MicroK8s Be sure no MicroK8s is installed, otherwise use sudo snap remove microk8s --purge . Install MicroK8s on Linux sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Enable DNS support microk8s enable dns Caution: For this GPU enabeling please do NOT use microk8s enable gpu . Actually this does not work on Arch, Ubuntu and all system we tried microk8s. Enable GPU support in microk8s Now we are adapting the following article . Add a demonset to the node which is responsible to enable gpu capacity (nvidia.com/gpu) for every node it is done once. microk8s kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.14/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml Add a label to the node for gpu support microk8s kubectl label nodes stefan-pc cloud.google.com/gke-accelerator=true Bugfix containerd gpu support for microk8s To enable the nvidia-conatiner-runtime in containerd edit the two files sudo vim /var/snap/microk8s/current/args/containerd-template.toml sudo vim /var/snap/microk8s/current/args/containerd.toml And replace # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" with # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] BinaryName = \"nvidia-container-runtime\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" With this setting every container executed by containerd has gpu support, but that is for our use case fine. Ensure nvidia driver is loaded and device files are ready Use the script from here . touch enable-dev-nvida.sh vim enable-dev-nvida.sh Add this code #!/bin/bash /sbin/modprobe nvidia if [ \"$?\" -eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\" -eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi And execute the script chmod +x enable-dev-nvida.sh sudo ./enable-dev-nvida.sh Also check that Nvidia and Cuda is present nvidia-smi Sample output +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.63.01 Driver Version: 470.63.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:3B:00.0 Off | N/A | | N/A 54C P0 N/A / N/A | 249MiB / 2002MiB | 10% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Restart MicroK8s and get node information microk8s stop && microk8s start && microk8s status --wait-ready microk8s kubectl describe node You should now get nvidia.com/gpu capacity like this Capacity: cpu: 8 ephemeral-storage: 473841544Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16030004Ki nvidia.com/gpu: 1 pods: 110 Test pod with GPU capabilities To test the gpu support we create a simple deployment yaml file vim test-gpu.yaml with content apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 requests: nvidia.com/gpu: 1 Now start the deployment and look for the results. This can take some time to pull the image. sudo microk8s kubectl apply -f test-gpu.yaml microk8s kubectl describe pod cuda-vector-add microk8s kubectl logs cuda-vector-add The successful log output shows that the GPU is working and can now be used. [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done","title":"Getting Started"},{"location":"started/#getting-started","text":"","title":"Getting Started"},{"location":"started/#install-microk8s-with-gpu-support","text":"","title":"Install MicroK8s with GPU support"},{"location":"started/#install-microk8s","text":"Be sure no MicroK8s is installed, otherwise use sudo snap remove microk8s --purge . Install MicroK8s on Linux sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Enable DNS support microk8s enable dns Caution: For this GPU enabeling please do NOT use microk8s enable gpu . Actually this does not work on Arch, Ubuntu and all system we tried microk8s.","title":"Install MicroK8s"},{"location":"started/#enable-gpu-support-in-microk8s","text":"Now we are adapting the following article . Add a demonset to the node which is responsible to enable gpu capacity (nvidia.com/gpu) for every node it is done once. microk8s kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.14/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml Add a label to the node for gpu support microk8s kubectl label nodes stefan-pc cloud.google.com/gke-accelerator=true","title":"Enable GPU support in microk8s"},{"location":"started/#bugfix-containerd-gpu-support-for-microk8s","text":"To enable the nvidia-conatiner-runtime in containerd edit the two files sudo vim /var/snap/microk8s/current/args/containerd-template.toml sudo vim /var/snap/microk8s/current/args/containerd.toml And replace # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" with # 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes' is a map from CRI RuntimeHandler strings, which specify types # of runtime configurations, to the matching configurations. # In this example, 'runc' is the RuntimeHandler string to match. [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] BinaryName = \"nvidia-container-runtime\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime] # runtime_type is the runtime type to use in containerd e.g. io.containerd.runtime.v1.linux runtime_type = \"io.containerd.runc.v1\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia-container-runtime.options] BinaryName = \"nvidia-container-runtime\" With this setting every container executed by containerd has gpu support, but that is for our use case fine.","title":"Bugfix containerd gpu support for microk8s"},{"location":"started/#ensure-nvidia-driver-is-loaded-and-device-files-are-ready","text":"Use the script from here . touch enable-dev-nvida.sh vim enable-dev-nvida.sh Add this code #!/bin/bash /sbin/modprobe nvidia if [ \"$?\" -eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\" -eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi And execute the script chmod +x enable-dev-nvida.sh sudo ./enable-dev-nvida.sh Also check that Nvidia and Cuda is present nvidia-smi Sample output +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.63.01 Driver Version: 470.63.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:3B:00.0 Off | N/A | | N/A 54C P0 N/A / N/A | 249MiB / 2002MiB | 10% Default | | | | N/A | +-------------------------------+----------------------+----------------------+","title":"Ensure nvidia driver is loaded and device files are ready"},{"location":"started/#restart-microk8s-and-get-node-information","text":"microk8s stop && microk8s start && microk8s status --wait-ready microk8s kubectl describe node You should now get nvidia.com/gpu capacity like this Capacity: cpu: 8 ephemeral-storage: 473841544Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16030004Ki nvidia.com/gpu: 1 pods: 110","title":"Restart MicroK8s and get node information"},{"location":"started/#test-pod-with-gpu-capabilities","text":"To test the gpu support we create a simple deployment yaml file vim test-gpu.yaml with content apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 requests: nvidia.com/gpu: 1 Now start the deployment and look for the results. This can take some time to pull the image. sudo microk8s kubectl apply -f test-gpu.yaml microk8s kubectl describe pod cuda-vector-add microk8s kubectl logs cuda-vector-add The successful log output shows that the GPU is working and can now be used. [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done","title":"Test pod with GPU capabilities"}]}